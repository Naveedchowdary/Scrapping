import os
import pysftp
import random
import zipfile
from time import sleep
from threading import Lock
from datetime import datetime
from selenium import webdriver
from openpyxl import load_workbook
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC

path_to_save_pdf = 'D:\\Downloaded County Documents\\AK-ANCHORAGE'

initiation_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
print("Initiation Time: ",initiation_time)

excel_write_lock = Lock()

chrome_options = Options()
chrome_options.add_experimental_option('prefs', {
    'plugins.always_open_pdf_externally': True,            # Give True to download & Give False to view
    'download.default_directory': path_to_save_pdf
})

excel_file = 'C:\\Scrappers\\County_dates.xlsx'  # Change the file extension to .xlsx
wb = load_workbook(excel_file)
ws = wb.active

counties = []
start_dates = []
end_dates = []
total_downloaded = []  # Add a list to store the total downloaded records

target_county = 'AK-ANCHORAGE'

# Assuming that the first row contains headers
headers = [cell.value for cell in ws[1]]

# Find the column indexes for 'County Name', 'Start Date', 'End Date', and 'Total_documents'
county_name_index = headers.index('County')
start_date_index = headers.index('Start_date')
end_date_index = headers.index('End_date')
total_documents_index = headers.index('Total_downloaded')

for row in ws.iter_rows(min_row=2):
    county_name = row[county_name_index].value
    if county_name == target_county:
        start_date = row[start_date_index].value
        end_date = row[end_date_index].value

        if start_date and end_date:
            counties.append(county_name)
            start_dates.append(start_date)  # Format date as needed
            end_dates.append(end_date)  # Format date as needed
            total_downloaded.append(row[total_documents_index].value)  # Append existing total downloaded records

if not counties:
    print("No valid data found in the Excel file.")
    exit()
    
d1 = start_date.strftime('%m/%d/%Y')
d2 = end_date.strftime('%m/%d/%Y')

browser = webdriver.Chrome(options=chrome_options)

url = "http://dnr.alaska.gov/ssd/recoff/"
browser.get(url)
browser.maximize_window()

sleep_duration = random.uniform(4,6)
sleep(sleep_duration)
search_records_frm_db = browser.find_element(By.XPATH,"//div/h4/a[contains(text(),'statewide database')]").click()
sleep_duration = random.uniform(4,6)
sleep(sleep_duration)
doc_type_search = browser.find_element(By.XPATH,"//a[text()='Document Type Search']").click()

def documents(docu_type):
    sleep_duration = random.uniform(4, 6)
    sleep(sleep_duration)

    print(f'About {docu_type}:')
    district = Select(browser.find_element(By.XPATH,"//select[@id='District']"))
    district.select_by_index('2')
    sleep_duration = random.uniform(4,6)
    sleep(sleep_duration)
    
    doc_type = Select(browser.find_element(By.XPATH,"//select[@id='IndexCode']"))
    doc_type.select_by_visible_text(docu_type)
    sleep_duration = random.uniform(4,6)
    sleep(sleep_duration)

    enter_date = browser.find_element(By.XPATH,"//input[@id='datepicker']").send_keys(d1)
    sleep_duration = random.uniform(4,6)
    sleep(sleep_duration)

    search =  browser.find_element(By.XPATH,"//button[@name='Document Type Search']").click()
    sleep_duration = random.uniform(10,15)
    sleep(sleep_duration)    
    
    # download
    total_pages = 0
    p = 0
    s = 0
    while True:
        sleep_duration = random.uniform(5,8)
        sleep(sleep_duration)
        p = p+1
        print('Scrapping page:',p)
        element_count = len(browser.find_elements(By.XPATH,"//table[@class='table table-bordered table-sm table-striped']/tbody"))
        total_pages += element_count
        
        records = browser.find_elements(By.XPATH,"//table[@class='table table-bordered table-sm table-striped']/tbody/tr/td[1]/a")
        clicked_doc_no = []
        if len(records)!=0:
            document_number = []
            for e in records:
                document_number.append(e.text)
            print('Total documents in page',p,':',len(document_number))
            d = 0
            
            for i in range(len(document_number)):
                doc_type_text =  browser.find_element(By.XPATH,f"//table[@class='table table-bordered table-sm table-striped']/tbody/tr[{i+1}]/td[4]").text
                if doc_type_text=='MORTGAGE' or doc_type_text=='DEED OF TRUST' or doc_type_text=='DEED OF TRUST HOME EQUITY LINE OF CREDIT':
                    doc_no =  browser.find_element(By.XPATH,f"//table[@class='table table-bordered table-sm table-striped']/tbody/tr/td[1]/a[contains(text(),'{document_number[i]}')]")
                    browser.execute_script("arguments[0].scrollIntoView(true);",doc_no)
                    sleep_duration = random.uniform(1,3)
                    sleep(sleep_duration)
                    doc_no.click()               
                    sleep_duration = random.uniform(4,6)
                    sleep(sleep_duration)
                    no = browser.find_element(By.XPATH,"//div[@id='card']/div/div[1]/div[1]/div/div[1]").text
                    clicked_doc_no.append(no)
                    try:
                        see_image = WebDriverWait(browser, 20).until(EC.element_to_be_clickable((By.XPATH, "//input[@value='See Image']")))
                        see_image.click()  
                        d+=1
                        sleep_duration = random.uniform(8,14)
                        sleep(sleep_duration)
                    except Exception as e:
                        pass
                    browser.back()
                    sleep_duration = random.uniform(4,6)
                    sleep(sleep_duration)

                elif doc_type_text=='DEED' or doc_type_text=='WARRANTY DEED' or doc_type_text=='STAT WARRANTY DEED' or doc_type_text=='SPECIAL WARRANTY DEED':
                    doc_no =  browser.find_element(By.XPATH,f"//table[@class='table table-bordered table-sm table-striped']/tbody/tr/td[1]/a[contains(text(),'{document_number[i]}')]")
                    browser.execute_script("arguments[0].scrollIntoView(true);",doc_no)
                    sleep_duration = random.uniform(1,3)
                    sleep(sleep_duration)
                    doc_no.click()               
                    sleep_duration = random.uniform(4,6)
                    sleep(sleep_duration)
                    no = browser.find_element(By.XPATH,"//div[@id='card']/div/div[1]/div[1]/div/div[1]").text
                    clicked_doc_no.append(no)
                    try:
                        see_image = WebDriverWait(browser, 20).until(EC.element_to_be_clickable((By.XPATH, "//input[@value='See Image']")))
                        see_image.click()   
                        d+=1
                        sleep_duration = random.uniform(8,14)
                        sleep(sleep_duration)
                    except Exception as e:
                        pass
                    browser.back()
                    sleep_duration = random.uniform(4,6)
                    sleep(sleep_duration)

                else:
                    pass
                sleep_duration = random.uniform(3,5)
                sleep(sleep_duration)
                
            print('Total downloadable documents:',len(clicked_doc_no))
            print('Downloaded document numbers:',clicked_doc_no)
            print('Successfully downloaded',d,'documents in page',p)
            
        else:
            d = 0
            print(f'{docu_type} documents are not available')
            
        s+=d
        sleep_duration = random.uniform(3,5)
        sleep(sleep_duration)
        break

    print('Total Pages: ',total_pages)
    print('Total documents:',len(clicked_doc_no))
    print('Total downloaded documents:',s)
    
    write_to_excel(target_county, s, len(clicked_doc_no))
    
    new_search = browser.find_element(By.XPATH,"//a[@id='navbarDropdown']").click()
    sleep_duration = random.uniform(2,4)
    sleep(sleep_duration)
    new_search_1 = browser.find_element(By.XPATH,"//div[@class='dropdown-menu show']/a[2][text()='Document Type Search']").click()
    sleep_duration = random.uniform(5,7)
    sleep(sleep_duration)
    
def write_to_excel(target_county, total_downloaded, total_recs):
    # Acquire the lock before writing to the Excel file
    with excel_write_lock:
        try:
            # Load the existing workbook
            wb = load_workbook(excel_file)
            ws = wb.active

            # Find the column indexes for headers
            headers = [cell.value for cell in ws[1]]
            total_documents_index = headers.index('Total_downloaded') + 1  # Add 1 for 1-based indexing
            total_recs_index = headers.index('Total_records') + 1
            
            # Find the row corresponding to the target county
            for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
                if row[0].value == target_county:  # Assuming target county is in the first column
                    # Update the 'Total_documents' column with the total downloaded documents
                    row[total_documents_index - 1].value = total_downloaded  # Subtract 1 for 0-based indexing
                    row[total_recs_index - 1].value = total_recs  # Subtract 1 for 0-based indexing
                    break

            # Save the workbook
            wb.save(excel_file)

        finally:
            # Release the lock after writing to the Excel file
            pass

documents('MORTGAGES')

#documents('DEEDS')

browser.quit()
 
end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
print("End Time: ",end_time)
